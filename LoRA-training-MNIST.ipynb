{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#usr/bin/env/python3\nimport torch as __T__\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import *\n__T__.manual_seed(1337)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T19:02:02.871458Z","iopub.execute_input":"2024-06-11T19:02:02.871802Z","iopub.status.idle":"2024-06-11T19:02:02.878983Z","shell.execute_reply.started":"2024-06-11T19:02:02.871776Z","shell.execute_reply":"2024-06-11T19:02:02.877949Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c7dce71b2b0>"},"metadata":{}}]},{"cell_type":"code","source":"#SVD ON RANK DEFICIENT MATRIX:\n__d : int = int(1e3)\n__k : int = int(1e3)\n\n__W_rank : int = int(1e2)\nW : __T__.cuda.DoubleTensor = __T__.randn(__d, __W_rank) @ __T__.randn(__W_rank, __k)\nprint(W)\nprint(f'\\n RANK OF MATRIX W : {np.linalg.matrix_rank(W)}')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:45:36.398668Z","iopub.execute_input":"2024-06-11T18:45:36.399381Z","iopub.status.idle":"2024-06-11T18:45:36.704066Z","shell.execute_reply.started":"2024-06-11T18:45:36.399344Z","shell.execute_reply":"2024-06-11T18:45:36.702745Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[  9.0497,  -3.1722,   6.0432,  ...,  -5.0396,  12.7957, -12.2203],\n        [-11.1418,  -5.9670,   7.7003,  ..., -10.3299,  10.8481,  10.5139],\n        [-21.7219,   0.4937,  15.5838,  ...,  -4.8988,  -5.0047,  -3.9311],\n        ...,\n        [  4.0619,  10.8808,   3.4869,  ..., -16.7343,  -2.7286,  16.4703],\n        [  2.2962,   2.5495,  -9.2799,  ...,   0.6069,   0.5630,   2.2210],\n        [  3.9346,   8.7400,  -2.9693,  ...,  13.3873,  -6.2420,   2.4494]])\n\n RANK OF MATRIX W : 100\n","output_type":"stream"}]},{"cell_type":"code","source":"U, S, V = __T__.svd(W)\n\n#RANK r FACTORIZATION \nU_r : __T__.cuda.DoubleTensor = U[:, : __W_rank]\nS_r : __T__.cuda.DoubleTensor = __T__.diag(S[: __W_rank])\nV_r : __T__.cuda.DoubleTensor = V[:, : __W_rank].t()\n\nB : __T__.cuda.DoubleTensor = U_r @ S_r\nA : __T__.cuda.DoubleTensor= V_r\nprint(f'SHAPE OF B : {B.shape}')\nprint(f'SHAPE OF A : {A.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:45:50.694470Z","iopub.execute_input":"2024-06-11T18:45:50.694851Z","iopub.status.idle":"2024-06-11T18:45:50.962700Z","shell.execute_reply.started":"2024-06-11T18:45:50.694822Z","shell.execute_reply":"2024-06-11T18:45:50.961716Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"SHAPE OF B : torch.Size([1000, 100])\nSHAPE OF A : torch.Size([100, 1000])\n","output_type":"stream"}]},{"cell_type":"code","source":"bias : __T__.cuda.DoubleTensor = __T__.randn(__d)\nx : __T__.cuda.DoubleTensor = __T__.randn(__d)\ny_org : __T__.cuda.DoubleTensor = W @ x + bias\ny_lord : __T__.cuda.DoubleTensor = (B @ A) @ x + bias\nprint(\"ORIGINAL y : \\n\", y_org)\nprint(\"LORDECOMPOSITION : \\n\", y_lord)\nprint(\"PARAMS FOR W : \", W.nelement())\nprint(\"PARAMS FOR LORDECOMPOSTION : \", B.nelement() + A.nelement())","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:45:54.987016Z","iopub.execute_input":"2024-06-11T18:45:54.987669Z","iopub.status.idle":"2024-06-11T18:45:55.023159Z","shell.execute_reply.started":"2024-06-11T18:45:54.987633Z","shell.execute_reply":"2024-06-11T18:45:55.022255Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"ORIGINAL y : \n tensor([ 5.7144e+01, -8.1995e+01, -1.4738e+02, -1.4767e+01, -7.6083e+02,\n         5.6425e+02,  2.4467e+02,  3.1799e+02, -1.0773e+02,  2.9801e+02,\n        -4.1974e+02,  1.9744e+02,  1.3310e+02,  6.8457e+01,  1.2593e+02,\n         2.4615e+02,  4.6234e+01,  8.6925e+02, -4.0376e+02,  5.1600e+02,\n         4.7942e+01, -5.4773e+01, -1.1973e+02, -3.3681e+02, -2.3289e+02,\n        -1.1506e+02,  1.1023e+02, -8.6167e+01,  3.5567e+02, -1.7758e+02,\n        -1.2409e+02,  3.9873e+02,  2.7278e+02, -2.4280e+01,  1.0292e+02,\n        -4.9170e+02,  8.0439e+00, -1.4529e+02,  1.0904e+02,  1.0627e+02,\n         1.3102e+02, -3.4549e+01, -1.8561e+02, -8.1543e+01, -3.0084e+02,\n         3.3054e+01,  6.4448e+01,  4.8099e+02, -1.3649e+02, -2.3533e+02,\n        -3.4932e+02, -1.8377e+02, -4.1827e+01, -3.7975e+02, -2.0687e+02,\n        -5.8278e+00, -7.4829e+02, -4.8551e+01,  5.8468e+01, -2.6475e+02,\n         1.9714e+02,  1.2015e+02,  1.0542e+02,  2.1464e+02,  6.1223e+01,\n         2.6325e+02,  1.9583e+02,  1.0796e+02, -5.7933e+02,  1.9031e+02,\n        -5.4414e+01,  3.5541e+02,  7.3470e+02,  2.3750e+02, -1.0324e+02,\n        -4.2282e+02,  5.0139e+02, -3.0878e+02, -5.4778e+02,  1.2622e+02,\n         1.1724e+02, -9.2912e+01,  1.5600e+02,  3.4463e+02, -2.9517e+01,\n        -1.1511e+02, -3.6381e+02, -7.9081e+00, -5.6644e+01, -1.4902e+02,\n        -3.5518e+01, -1.7646e+02, -1.2954e+02, -1.4959e+02, -2.3924e+02,\n         1.2787e+02, -4.4471e+02, -2.0439e+01,  1.1911e+02, -5.3896e+01,\n        -5.8257e+02, -2.9726e+02,  4.9893e+02,  5.5255e+02, -4.0767e+02,\n         2.1650e+02, -2.0335e+02,  2.8874e+02,  9.9576e+01, -1.8458e+02,\n        -7.5883e+01, -1.3725e+02, -6.7046e+02, -1.5122e+02,  2.7524e+02,\n        -1.4326e+02, -9.1558e+01, -5.8957e+01,  4.5151e+01, -1.7991e+02,\n        -1.7849e+02,  3.1744e+02,  6.1801e+02, -2.5198e+02,  4.6387e+02,\n        -9.1028e+01, -1.1525e+00, -2.8918e+02, -6.1835e+02,  5.8981e+02,\n        -4.1772e+02,  1.4730e+02, -2.7093e+02, -3.6179e+02, -8.2813e+01,\n         2.5127e+02, -3.8534e+02, -2.4374e+01,  4.6354e+02,  2.9072e+02,\n         5.9506e+02, -2.0897e+02,  1.1127e+02,  2.0445e+01, -7.3987e+01,\n        -1.9297e+02,  1.8469e+02,  1.2911e+02, -1.0568e+02, -3.0595e+02,\n        -4.9894e+01, -4.2032e+01,  2.2700e+02, -1.2328e+02,  3.0977e+02,\n        -5.2756e+02, -3.8780e+02,  2.5206e+02,  1.6180e+02,  1.0498e+02,\n        -1.5130e+02,  1.5770e+02, -1.6551e+02, -2.5177e+02,  1.7812e+02,\n        -2.6127e+02,  4.6816e+00, -2.3744e+02, -1.1260e+02,  3.6051e+02,\n        -1.5322e+02, -3.0609e+02, -7.7982e+01,  5.4552e+02,  5.3910e+02,\n         8.6320e+01,  2.0980e+02,  5.6665e+01,  3.1010e+02, -3.3123e+02,\n         3.1197e+02,  1.2405e+02,  9.0627e+01, -1.1669e+01,  5.6740e+01,\n         1.1176e+02,  2.7798e+02, -2.6471e+01, -3.7728e+01, -1.1811e+02,\n         2.5893e+01,  2.6670e+01,  3.7590e+02, -2.6792e+02, -1.4504e+02,\n         1.6371e+02,  8.1150e+01,  1.9933e+02,  7.0742e+02, -3.1176e+02,\n         3.2087e+01, -5.6891e+01,  3.1345e+01,  5.0144e+01, -1.4292e+02,\n        -6.6275e+02, -7.2495e+01,  2.6135e+02,  3.2369e+02, -3.9716e+01,\n        -2.3008e+02,  2.8820e+02,  1.1981e+02,  1.1742e+02, -5.4038e+02,\n        -3.1577e+02,  1.5452e+02,  2.9906e+02,  7.8525e+02,  9.5768e+00,\n         5.3664e+01,  3.0234e+02,  4.5216e+02, -3.0507e+02, -1.5071e+01,\n         3.6048e+01, -3.7630e+02,  8.7812e+01,  2.9581e+02,  3.5912e+01,\n         1.0488e+02,  1.1571e+01,  4.1796e+01,  3.2248e+02,  1.1026e+02,\n         4.1216e+02,  1.2852e+02,  1.2823e+02, -3.1214e+02, -2.3599e+02,\n        -1.6729e+01,  2.2616e+01,  8.2886e+02,  1.1563e+02, -3.9112e+02,\n        -2.2107e+02, -1.6729e+02,  1.3610e+02, -3.0270e+02,  1.7709e+02,\n         1.1430e+02, -4.0682e+02,  1.0190e+02, -1.3708e+02, -1.6935e+02,\n         1.9126e+02,  4.9385e+01,  5.2279e+02,  2.0367e+02, -3.3539e+01,\n        -3.5545e+01, -2.7428e+02,  2.1494e+02, -3.0063e-01, -1.3274e+02,\n         1.7194e+02, -3.6744e+02,  1.0578e+02, -1.0577e+02, -2.8585e+02,\n         3.9945e+02,  1.0854e+02, -1.3575e+01, -4.5562e+02, -1.3806e+01,\n         8.4395e+01,  1.6478e+02, -1.9936e+02, -2.1739e+02,  2.0244e+02,\n         1.9134e+02, -1.1460e+03, -1.0882e+02, -2.6349e+02,  4.4202e+02,\n         5.9967e+02,  5.0122e+02, -1.5349e+02, -1.1912e+02, -2.6309e+02,\n        -3.9286e+01,  2.7273e+02, -6.3759e+02, -1.8648e+01, -4.5737e+02,\n        -2.1828e+02,  1.8002e+02, -2.1195e+02,  7.6979e+01, -8.3630e+01,\n         4.6817e+02,  2.4414e+02,  2.0660e+02, -1.7707e+02,  5.0362e+02,\n         2.4896e+02, -1.5664e+02, -5.2846e+02,  9.3783e+01, -1.8842e+02,\n         1.6301e+02,  1.1698e+02,  1.9511e+02, -3.9397e+01, -2.4714e+02,\n        -2.0606e+02,  3.6810e+02,  6.2537e+01,  2.7892e+02,  1.4205e+02,\n        -1.1415e+02,  3.2757e+02, -5.7917e+01,  1.4117e+02, -6.6200e+01,\n        -1.4108e+02, -4.2988e+02, -6.7852e+02, -5.4853e+01, -5.8205e+02,\n         3.6087e+02,  2.8484e+02, -6.1523e+02,  4.2589e+02,  1.6379e+02,\n         4.0977e+01,  6.2484e+01,  1.5113e+02,  2.0645e+02, -3.4795e+02,\n         2.7510e+02,  6.6084e+01, -3.1200e+02,  3.5269e+02,  1.5874e+02,\n        -3.0377e+02, -4.1191e+02, -2.1317e+02,  2.7928e+02, -9.5961e+01,\n        -3.5056e+02, -5.4636e+02,  7.8300e+00, -1.9884e+02,  3.9047e+02,\n        -3.6726e+02,  1.0062e+02,  6.1327e+02, -5.1202e+02,  6.1134e+02,\n        -5.8615e+02, -1.8231e+02, -6.3507e+01,  5.2693e+01, -3.0569e+02,\n         5.1905e+01,  2.0832e+02,  4.4078e+02,  1.8712e+02, -5.8443e+01,\n         2.1185e+02, -5.4675e+02,  3.0379e+02, -1.8783e+02,  3.7431e+02,\n         2.3010e+01,  3.8412e+02, -1.8953e+02, -2.2242e+01,  1.8314e+02,\n        -6.1007e+01,  1.7622e+01, -5.5958e+02,  4.1235e+02, -8.8698e+01,\n         1.5887e+02,  2.7022e+02,  3.0980e+02, -2.6039e+02, -3.9319e+02,\n        -5.8211e+00,  1.5385e+02, -3.3703e+01,  1.0188e+02,  3.3713e+02,\n        -2.8022e+02,  1.6919e+02, -5.3370e+02,  4.6472e+01,  2.4230e+02,\n        -3.6816e+02,  6.1096e+02, -3.0978e+01, -1.8729e+02, -8.7063e+01,\n         2.1784e+02, -7.2338e+02,  2.1353e+02, -3.4148e+02,  2.5483e+02,\n         2.3437e+02, -3.8735e+02, -2.6991e+02, -4.0174e+02,  1.1375e+02,\n        -3.1505e+02, -4.2165e+00, -5.4011e+01,  2.0068e+02, -9.6944e+01,\n        -4.1866e+02, -6.3527e+02,  1.8848e+02,  1.7143e+00, -7.0524e+01,\n        -1.4382e+02, -3.2926e+02,  2.3812e+02,  1.9558e+01,  1.3582e+02,\n         9.1182e+01, -1.3734e+02,  4.6220e+01, -8.2266e+01,  5.8041e+01,\n        -2.3334e+02, -1.7876e+02,  2.6468e+02, -1.7211e+02,  1.2260e+01,\n        -1.8026e+02,  3.8806e+02, -1.8427e+02,  3.0784e+02, -1.0670e+02,\n         6.4946e+01,  1.0150e+02, -3.4033e+02,  3.3966e+02,  2.5151e+02,\n        -1.3972e+02, -3.3232e+01, -2.4563e+01,  1.9981e+02,  1.2641e+01,\n        -2.9309e+02, -2.7146e+02, -5.4751e+02,  9.1852e+01, -6.6987e+01,\n        -1.2946e+02, -2.9033e+02, -1.8362e+02, -2.2729e+02, -5.7151e+02,\n        -1.8555e+02, -1.5722e+02, -1.1719e+02, -7.7645e+00,  9.2568e-01,\n        -3.5442e+00,  3.0868e+02,  2.5297e+02,  4.3692e+02,  1.6761e+02,\n        -1.8986e+02,  2.0447e+02, -4.5182e-01, -2.3549e+00, -4.6470e+01,\n        -4.0199e+02,  8.6323e+01, -1.3358e+02, -5.6351e+02, -3.5383e+02,\n        -5.8766e+01,  1.5982e+02, -5.1217e+02,  6.1171e+01,  6.8991e+02,\n        -3.4041e+02,  8.7605e+00, -1.0346e+03, -3.8757e+02, -1.5543e+02,\n        -1.4617e+01, -1.1596e+02, -2.3380e+02, -3.2622e+02,  4.7948e+02,\n        -2.6559e+02,  8.9197e+01, -7.7431e+00, -7.3293e+01, -2.2534e+02,\n         6.1082e+01,  1.2611e+02, -1.3589e+02,  7.1409e+02, -1.6306e+02,\n        -2.4617e+02,  4.1287e+01, -9.9939e+01, -1.7329e+02, -8.6556e+01,\n         2.8301e+02,  4.6266e+02,  3.1884e+02, -6.1567e+02, -1.8069e+02,\n        -1.5890e+02, -1.9196e+02,  1.5240e+02,  3.7568e+02,  2.5012e+02,\n         3.4265e+01,  2.3116e+02,  1.2221e+02, -7.6633e+02,  8.8926e+01,\n        -5.3161e+01, -9.9256e+01,  1.8827e+02,  1.2374e+02, -6.7283e+01,\n        -3.8214e+02, -1.9209e+02, -3.5585e+01,  2.4507e+02,  3.3100e+02,\n        -1.0286e+02, -2.1855e+02, -3.8368e+02, -7.3359e+01, -2.2887e+01,\n        -2.5580e+02,  1.0911e+02, -3.1155e+02, -3.1050e+02, -2.5434e+02,\n         1.0337e+02,  1.3605e+02, -8.9856e+01, -4.5620e+02,  3.2615e+02,\n         2.8594e+02, -6.7203e+02, -1.7329e+02,  4.9731e+02, -4.5927e+01,\n        -2.9469e+02, -6.7879e+02,  7.0723e+02, -1.1216e+02, -4.6186e+02,\n         5.2944e+02, -4.2005e+01, -1.2055e+02, -6.2643e+01, -7.6022e+01,\n        -3.9121e+02, -2.5938e+02,  7.5974e+00,  8.2695e+01,  9.6756e-01,\n         4.8895e+02,  3.6520e+02, -9.0334e+01, -2.2206e+02,  7.8269e+01,\n        -2.9078e+02,  1.2510e+02, -1.2891e+02,  4.3029e+02, -2.9750e+02,\n         2.8579e+01,  7.2667e+01,  4.9827e+01,  5.3851e+01,  3.3355e+02,\n        -1.2832e+02,  2.3291e+02, -3.5308e+02,  1.7936e+01, -1.5868e+02,\n         8.3503e+01, -1.4901e+02, -2.6377e+01, -2.3254e+02, -1.2587e+02,\n        -5.9691e+02,  1.1212e+02,  6.1358e-02, -5.8180e+02,  3.8849e+02,\n         3.7292e+01,  4.1571e+02, -7.4397e+02, -2.9722e+02, -1.7182e+02,\n        -1.2449e+02, -2.2961e+02,  1.0642e+02, -2.8283e+02,  1.1253e+02,\n         1.4819e+02, -2.5120e+02, -1.8256e+02, -3.1537e+02, -3.5918e+02,\n        -1.3727e+01, -2.5256e+02, -3.3951e+02,  4.8042e+02, -3.7319e+02,\n         5.1413e+02, -5.6166e+02, -7.9882e+01,  2.6003e+02, -2.2188e+02,\n        -1.0177e+02,  3.5271e+02,  2.3384e+02,  6.0992e+01, -5.5559e+01,\n         1.9678e+02, -3.2434e+02,  3.3623e+01,  3.8089e+01, -3.0266e+02,\n        -1.8918e+02, -3.1818e+01,  1.4336e+02,  1.5760e+02,  2.4749e+01,\n        -3.7001e+02, -2.1663e+02, -7.0489e+01,  6.7122e+02,  6.9709e+01,\n         3.0499e+02,  4.1045e+02,  3.3742e+02,  2.3481e+02, -4.1465e+02,\n        -3.8656e+02,  5.3958e+01, -9.5931e+01,  4.0913e+02, -3.4359e+01,\n        -1.3261e+02, -4.2297e+02, -1.8751e+02,  8.1487e+00,  7.9376e+01,\n         2.1206e+01, -8.7214e+01,  2.1350e+02,  9.6179e+01, -4.4604e+02,\n        -5.2157e+02, -7.2022e+01,  2.8809e+02,  2.1576e+02,  4.1632e+02,\n        -4.5931e+02,  7.3227e+00, -1.4251e+02,  1.7137e+02,  2.1584e+02,\n         8.0311e+01,  1.8427e+02, -3.5661e+02, -2.5036e+02, -3.0235e+02,\n         1.4116e+02, -5.0341e+02, -1.1217e+01,  2.1780e+02,  1.6231e+02,\n         4.0349e+02,  2.4140e+02, -5.2571e+02, -5.1638e+02, -5.5183e+02,\n        -4.1269e+02, -8.4020e+02,  1.4413e+02, -3.4188e+01, -4.4450e+02,\n        -5.9636e+02,  9.4113e+01,  5.7062e+01,  2.3626e+02, -4.2296e+02,\n         3.2550e+02,  4.2906e+02,  4.0226e+01,  1.9970e+02, -2.9531e+00,\n         2.1471e+01,  6.4368e+01, -4.4824e+01, -1.0677e+03, -2.2907e+01,\n         2.8409e+02,  7.2714e+01,  1.3117e+02,  7.0831e+01,  1.7393e+02,\n         2.6355e+02, -2.8288e+02, -5.7320e+01, -1.4530e+02,  1.8223e+02,\n         2.8651e+01, -1.6598e+02,  3.6831e+01,  1.0323e+02,  2.1919e+02,\n        -6.9993e+01, -7.4922e+01,  1.4479e+02, -1.2069e+02,  2.8983e+02,\n        -6.7180e+01,  1.0231e+02, -1.6203e+02,  1.9782e+02, -1.6038e+02,\n        -2.5145e+02,  4.0866e+02,  3.1632e+02,  1.6956e+02, -1.3674e+02,\n        -4.7412e+01, -3.5761e+01,  1.7170e+02,  5.0292e+02, -5.1491e+01,\n         3.5725e+02,  1.0711e+02,  2.8948e+02,  1.2529e+02,  8.8769e+01,\n        -2.3818e+01, -3.3957e+02, -2.2694e+02, -1.5477e+02,  1.9022e+02,\n         2.5652e+02, -3.5102e+02, -5.3961e+02, -2.5266e+02, -1.3476e+02,\n        -2.2623e+02, -4.6485e+02, -1.7087e+02,  1.7975e+02,  1.4829e+02,\n         2.1216e+02, -5.7253e+02, -6.8831e+01, -3.7502e+02,  3.3081e+02,\n        -9.5422e+01,  1.4420e+02, -1.9829e+02,  9.2289e+02, -3.2843e+02,\n         2.5883e+01, -2.6407e+02, -4.6668e+02, -1.7423e+02,  5.1035e+02,\n        -1.5381e+02,  9.6053e+01,  1.1902e+02, -3.7858e+02,  2.1505e+02,\n        -2.2044e+02,  3.4774e+02, -2.5803e+02, -3.2431e+00, -1.4174e+02,\n         2.7708e+02, -1.0084e+02,  1.9423e+02,  9.7486e+01,  3.1458e+02,\n        -3.3662e+00, -2.2021e+02,  2.8919e+02,  9.8252e+01, -2.8699e+02,\n         2.5109e+02,  9.1456e+01,  5.6747e+01,  2.4632e+02, -3.7540e+02,\n         9.1402e+00,  5.3820e+01,  4.2743e+02,  3.4381e+02, -1.0348e+02,\n        -4.4070e+02,  1.2836e+02, -1.6284e+02, -1.3297e+02, -6.4355e+01,\n        -5.2239e+02,  8.1618e+01,  3.2284e+02,  2.8020e+01,  2.5904e+02,\n         1.9759e+02,  1.1258e+02, -1.9020e+02, -1.8467e+02,  9.1500e+01,\n         3.2305e+01,  1.0598e+02, -5.7936e+01,  1.6846e+02,  2.9682e+02,\n        -2.3748e+02,  3.5622e+02, -1.0048e+02, -8.8013e+01, -8.0007e+01,\n         3.8925e+02, -3.2770e+01,  4.6019e+01, -5.7634e+01,  4.4842e+02,\n        -1.6580e+02, -2.8304e+02, -5.0545e+02,  1.9386e+02, -2.8786e+02,\n         1.7045e+02, -2.7340e+02, -2.3974e+02,  5.8846e+02, -2.3319e+02,\n        -1.9619e+02, -5.1924e+01, -2.2796e+02, -5.8598e+02,  3.6916e+02,\n        -7.5199e+00,  9.2594e+01,  7.8712e+01,  2.5771e+01,  5.6311e+02,\n         6.2808e+02,  1.1905e+02,  5.8302e+01, -2.3574e+02, -1.7128e+02,\n        -2.7178e+01, -4.8334e+01,  1.9204e+02, -3.5789e+02, -2.5134e+02,\n        -1.5710e+01, -3.4543e+02,  2.0201e+02, -7.7125e+01,  8.8916e+01,\n         3.3458e+02,  2.5693e+02, -1.1466e+02, -4.8138e+02, -1.1061e+02,\n         1.4103e+02,  2.0248e+02, -4.6693e+02,  1.0515e+02,  3.1382e+02,\n         7.1360e+02,  1.6668e+02,  2.6694e+02, -2.0838e+02,  1.2243e+02,\n        -3.7100e+02, -1.3930e+02, -1.2251e+02, -8.3206e+01, -2.8321e+02,\n         1.3937e+02, -2.4800e+02,  3.0903e+02,  3.9903e+02,  6.0665e+01,\n        -1.3783e+02,  1.1732e+02, -1.6002e+02,  2.5998e+02,  5.6101e+01,\n        -7.1485e+00, -1.7955e+02, -2.7303e+02,  8.9997e+01, -2.8676e+02,\n         1.1971e+02,  2.2701e+02,  2.0217e+02,  3.4362e+02,  2.0539e+02,\n         4.5421e+02, -9.1052e+01,  5.0561e+01,  1.2689e+02, -1.3639e+02,\n         1.2318e+02, -3.5519e+02, -4.6309e+02,  2.6260e+02, -5.0717e+00,\n        -2.1670e+02,  1.4080e+02, -4.1876e+02, -1.7712e+02, -1.0961e+02,\n         2.1117e+02,  6.0087e+02,  1.5979e+02, -3.1308e+02,  3.8196e+01,\n         3.3052e+02, -1.5378e+02, -3.7169e+02,  1.0073e+02, -4.4185e+01,\n        -5.2283e+02,  2.7499e+02,  3.7366e+02,  5.0785e+01, -1.8691e+02,\n         1.0298e+00,  4.4110e+02,  1.9848e+02,  3.9934e+02,  2.2982e+02,\n         3.5545e+02,  2.2001e+02,  2.3101e+02, -1.5403e+02, -4.1422e+02,\n        -1.1263e+02,  1.0499e+02,  3.0001e+02,  1.3146e+02,  4.5125e+02,\n        -3.7438e+02,  7.9138e+00, -1.3654e+01, -1.2665e+02, -6.5341e+01,\n         8.5399e+02,  6.5139e+01, -7.1417e+01,  3.9979e+02, -1.6059e+02,\n        -5.5508e+02, -1.0705e+02,  1.5273e+02,  1.8095e+02, -1.0104e+02,\n         5.2482e+02,  2.1674e+02, -2.6014e+02,  1.6277e+02, -4.4570e+02,\n        -2.4823e+02,  2.4940e+02, -3.1398e+02, -8.5458e+01,  2.2945e+02,\n         3.1343e+02,  2.9559e+02,  3.3557e+02,  2.8838e+01,  3.0616e+02,\n         8.8445e+01,  1.0080e+01,  2.1245e+02,  9.3967e+01,  9.4808e+00])\nLORDECOMPOSITION : \n tensor([ 5.7146e+01, -8.1997e+01, -1.4738e+02, -1.4768e+01, -7.6083e+02,\n         5.6425e+02,  2.4467e+02,  3.1799e+02, -1.0773e+02,  2.9802e+02,\n        -4.1974e+02,  1.9744e+02,  1.3310e+02,  6.8456e+01,  1.2593e+02,\n         2.4615e+02,  4.6233e+01,  8.6925e+02, -4.0376e+02,  5.1600e+02,\n         4.7942e+01, -5.4774e+01, -1.1974e+02, -3.3681e+02, -2.3289e+02,\n        -1.1506e+02,  1.1023e+02, -8.6167e+01,  3.5567e+02, -1.7758e+02,\n        -1.2409e+02,  3.9873e+02,  2.7278e+02, -2.4280e+01,  1.0292e+02,\n        -4.9170e+02,  8.0434e+00, -1.4529e+02,  1.0904e+02,  1.0627e+02,\n         1.3102e+02, -3.4548e+01, -1.8561e+02, -8.1543e+01, -3.0084e+02,\n         3.3051e+01,  6.4448e+01,  4.8099e+02, -1.3649e+02, -2.3533e+02,\n        -3.4932e+02, -1.8377e+02, -4.1827e+01, -3.7975e+02, -2.0687e+02,\n        -5.8284e+00, -7.4829e+02, -4.8551e+01,  5.8468e+01, -2.6475e+02,\n         1.9714e+02,  1.2015e+02,  1.0542e+02,  2.1464e+02,  6.1224e+01,\n         2.6325e+02,  1.9583e+02,  1.0796e+02, -5.7933e+02,  1.9031e+02,\n        -5.4413e+01,  3.5541e+02,  7.3470e+02,  2.3749e+02, -1.0324e+02,\n        -4.2282e+02,  5.0139e+02, -3.0878e+02, -5.4778e+02,  1.2622e+02,\n         1.1724e+02, -9.2912e+01,  1.5600e+02,  3.4463e+02, -2.9516e+01,\n        -1.1511e+02, -3.6381e+02, -7.9081e+00, -5.6643e+01, -1.4902e+02,\n        -3.5519e+01, -1.7646e+02, -1.2954e+02, -1.4960e+02, -2.3924e+02,\n         1.2787e+02, -4.4471e+02, -2.0440e+01,  1.1911e+02, -5.3896e+01,\n        -5.8257e+02, -2.9726e+02,  4.9893e+02,  5.5255e+02, -4.0767e+02,\n         2.1650e+02, -2.0335e+02,  2.8874e+02,  9.9576e+01, -1.8458e+02,\n        -7.5883e+01, -1.3725e+02, -6.7046e+02, -1.5122e+02,  2.7524e+02,\n        -1.4326e+02, -9.1557e+01, -5.8958e+01,  4.5151e+01, -1.7991e+02,\n        -1.7849e+02,  3.1744e+02,  6.1801e+02, -2.5198e+02,  4.6387e+02,\n        -9.1028e+01, -1.1524e+00, -2.8918e+02, -6.1835e+02,  5.8981e+02,\n        -4.1772e+02,  1.4730e+02, -2.7094e+02, -3.6179e+02, -8.2812e+01,\n         2.5127e+02, -3.8534e+02, -2.4374e+01,  4.6354e+02,  2.9072e+02,\n         5.9506e+02, -2.0897e+02,  1.1127e+02,  2.0444e+01, -7.3987e+01,\n        -1.9297e+02,  1.8469e+02,  1.2911e+02, -1.0568e+02, -3.0595e+02,\n        -4.9894e+01, -4.2032e+01,  2.2700e+02, -1.2328e+02,  3.0977e+02,\n        -5.2756e+02, -3.8780e+02,  2.5206e+02,  1.6180e+02,  1.0498e+02,\n        -1.5130e+02,  1.5770e+02, -1.6551e+02, -2.5177e+02,  1.7812e+02,\n        -2.6127e+02,  4.6814e+00, -2.3744e+02, -1.1260e+02,  3.6051e+02,\n        -1.5322e+02, -3.0609e+02, -7.7982e+01,  5.4553e+02,  5.3910e+02,\n         8.6320e+01,  2.0980e+02,  5.6665e+01,  3.1010e+02, -3.3123e+02,\n         3.1197e+02,  1.2405e+02,  9.0627e+01, -1.1669e+01,  5.6740e+01,\n         1.1176e+02,  2.7798e+02, -2.6471e+01, -3.7728e+01, -1.1811e+02,\n         2.5894e+01,  2.6669e+01,  3.7590e+02, -2.6792e+02, -1.4504e+02,\n         1.6371e+02,  8.1150e+01,  1.9933e+02,  7.0742e+02, -3.1176e+02,\n         3.2087e+01, -5.6891e+01,  3.1345e+01,  5.0143e+01, -1.4292e+02,\n        -6.6275e+02, -7.2495e+01,  2.6135e+02,  3.2369e+02, -3.9716e+01,\n        -2.3009e+02,  2.8820e+02,  1.1981e+02,  1.1742e+02, -5.4039e+02,\n        -3.1577e+02,  1.5452e+02,  2.9906e+02,  7.8525e+02,  9.5771e+00,\n         5.3664e+01,  3.0234e+02,  4.5216e+02, -3.0507e+02, -1.5071e+01,\n         3.6048e+01, -3.7630e+02,  8.7812e+01,  2.9581e+02,  3.5912e+01,\n         1.0488e+02,  1.1571e+01,  4.1796e+01,  3.2248e+02,  1.1026e+02,\n         4.1216e+02,  1.2852e+02,  1.2823e+02, -3.1214e+02, -2.3599e+02,\n        -1.6729e+01,  2.2616e+01,  8.2886e+02,  1.1563e+02, -3.9112e+02,\n        -2.2107e+02, -1.6729e+02,  1.3610e+02, -3.0270e+02,  1.7709e+02,\n         1.1430e+02, -4.0682e+02,  1.0190e+02, -1.3708e+02, -1.6935e+02,\n         1.9126e+02,  4.9385e+01,  5.2279e+02,  2.0367e+02, -3.3538e+01,\n        -3.5545e+01, -2.7428e+02,  2.1494e+02, -3.0050e-01, -1.3274e+02,\n         1.7195e+02, -3.6744e+02,  1.0578e+02, -1.0577e+02, -2.8585e+02,\n         3.9945e+02,  1.0854e+02, -1.3575e+01, -4.5562e+02, -1.3806e+01,\n         8.4395e+01,  1.6478e+02, -1.9936e+02, -2.1739e+02,  2.0244e+02,\n         1.9135e+02, -1.1460e+03, -1.0882e+02, -2.6349e+02,  4.4202e+02,\n         5.9967e+02,  5.0122e+02, -1.5349e+02, -1.1912e+02, -2.6309e+02,\n        -3.9287e+01,  2.7273e+02, -6.3759e+02, -1.8648e+01, -4.5737e+02,\n        -2.1829e+02,  1.8002e+02, -2.1195e+02,  7.6980e+01, -8.3630e+01,\n         4.6817e+02,  2.4414e+02,  2.0660e+02, -1.7707e+02,  5.0362e+02,\n         2.4896e+02, -1.5664e+02, -5.2846e+02,  9.3783e+01, -1.8842e+02,\n         1.6301e+02,  1.1698e+02,  1.9511e+02, -3.9397e+01, -2.4714e+02,\n        -2.0607e+02,  3.6811e+02,  6.2537e+01,  2.7892e+02,  1.4205e+02,\n        -1.1415e+02,  3.2757e+02, -5.7917e+01,  1.4117e+02, -6.6200e+01,\n        -1.4108e+02, -4.2988e+02, -6.7853e+02, -5.4853e+01, -5.8205e+02,\n         3.6087e+02,  2.8484e+02, -6.1523e+02,  4.2589e+02,  1.6379e+02,\n         4.0976e+01,  6.2485e+01,  1.5113e+02,  2.0645e+02, -3.4795e+02,\n         2.7510e+02,  6.6083e+01, -3.1200e+02,  3.5269e+02,  1.5874e+02,\n        -3.0377e+02, -4.1192e+02, -2.1317e+02,  2.7928e+02, -9.5960e+01,\n        -3.5056e+02, -5.4636e+02,  7.8309e+00, -1.9884e+02,  3.9047e+02,\n        -3.6726e+02,  1.0062e+02,  6.1327e+02, -5.1202e+02,  6.1134e+02,\n        -5.8615e+02, -1.8231e+02, -6.3507e+01,  5.2694e+01, -3.0569e+02,\n         5.1905e+01,  2.0832e+02,  4.4078e+02,  1.8712e+02, -5.8443e+01,\n         2.1184e+02, -5.4675e+02,  3.0379e+02, -1.8783e+02,  3.7431e+02,\n         2.3011e+01,  3.8412e+02, -1.8953e+02, -2.2242e+01,  1.8314e+02,\n        -6.1007e+01,  1.7622e+01, -5.5959e+02,  4.1235e+02, -8.8698e+01,\n         1.5887e+02,  2.7022e+02,  3.0980e+02, -2.6039e+02, -3.9319e+02,\n        -5.8210e+00,  1.5385e+02, -3.3703e+01,  1.0188e+02,  3.3713e+02,\n        -2.8022e+02,  1.6919e+02, -5.3370e+02,  4.6472e+01,  2.4230e+02,\n        -3.6815e+02,  6.1096e+02, -3.0977e+01, -1.8729e+02, -8.7063e+01,\n         2.1784e+02, -7.2338e+02,  2.1353e+02, -3.4148e+02,  2.5483e+02,\n         2.3437e+02, -3.8735e+02, -2.6991e+02, -4.0174e+02,  1.1375e+02,\n        -3.1505e+02, -4.2162e+00, -5.4011e+01,  2.0068e+02, -9.6944e+01,\n        -4.1866e+02, -6.3527e+02,  1.8848e+02,  1.7143e+00, -7.0525e+01,\n        -1.4382e+02, -3.2926e+02,  2.3812e+02,  1.9558e+01,  1.3582e+02,\n         9.1182e+01, -1.3734e+02,  4.6221e+01, -8.2266e+01,  5.8040e+01,\n        -2.3334e+02, -1.7876e+02,  2.6469e+02, -1.7211e+02,  1.2260e+01,\n        -1.8026e+02,  3.8806e+02, -1.8427e+02,  3.0784e+02, -1.0670e+02,\n         6.4945e+01,  1.0151e+02, -3.4033e+02,  3.3966e+02,  2.5151e+02,\n        -1.3972e+02, -3.3231e+01, -2.4563e+01,  1.9981e+02,  1.2641e+01,\n        -2.9309e+02, -2.7146e+02, -5.4751e+02,  9.1852e+01, -6.6988e+01,\n        -1.2946e+02, -2.9033e+02, -1.8362e+02, -2.2729e+02, -5.7151e+02,\n        -1.8556e+02, -1.5722e+02, -1.1719e+02, -7.7645e+00,  9.2541e-01,\n        -3.5440e+00,  3.0868e+02,  2.5297e+02,  4.3692e+02,  1.6761e+02,\n        -1.8986e+02,  2.0447e+02, -4.5207e-01, -2.3545e+00, -4.6471e+01,\n        -4.0199e+02,  8.6324e+01, -1.3358e+02, -5.6351e+02, -3.5383e+02,\n        -5.8766e+01,  1.5982e+02, -5.1217e+02,  6.1172e+01,  6.8991e+02,\n        -3.4041e+02,  8.7604e+00, -1.0347e+03, -3.8757e+02, -1.5543e+02,\n        -1.4617e+01, -1.1596e+02, -2.3380e+02, -3.2622e+02,  4.7948e+02,\n        -2.6559e+02,  8.9197e+01, -7.7431e+00, -7.3293e+01, -2.2534e+02,\n         6.1083e+01,  1.2611e+02, -1.3589e+02,  7.1409e+02, -1.6306e+02,\n        -2.4617e+02,  4.1287e+01, -9.9940e+01, -1.7329e+02, -8.6556e+01,\n         2.8301e+02,  4.6266e+02,  3.1884e+02, -6.1567e+02, -1.8069e+02,\n        -1.5890e+02, -1.9196e+02,  1.5240e+02,  3.7568e+02,  2.5012e+02,\n         3.4265e+01,  2.3116e+02,  1.2221e+02, -7.6633e+02,  8.8926e+01,\n        -5.3160e+01, -9.9255e+01,  1.8827e+02,  1.2374e+02, -6.7283e+01,\n        -3.8214e+02, -1.9209e+02, -3.5585e+01,  2.4507e+02,  3.3100e+02,\n        -1.0287e+02, -2.1856e+02, -3.8368e+02, -7.3359e+01, -2.2888e+01,\n        -2.5580e+02,  1.0911e+02, -3.1155e+02, -3.1050e+02, -2.5434e+02,\n         1.0337e+02,  1.3605e+02, -8.9855e+01, -4.5620e+02,  3.2615e+02,\n         2.8594e+02, -6.7203e+02, -1.7329e+02,  4.9731e+02, -4.5927e+01,\n        -2.9469e+02, -6.7879e+02,  7.0723e+02, -1.1216e+02, -4.6186e+02,\n         5.2944e+02, -4.2005e+01, -1.2055e+02, -6.2644e+01, -7.6023e+01,\n        -3.9122e+02, -2.5938e+02,  7.5974e+00,  8.2695e+01,  9.6756e-01,\n         4.8895e+02,  3.6520e+02, -9.0335e+01, -2.2206e+02,  7.8269e+01,\n        -2.9078e+02,  1.2510e+02, -1.2891e+02,  4.3029e+02, -2.9750e+02,\n         2.8578e+01,  7.2667e+01,  4.9828e+01,  5.3851e+01,  3.3355e+02,\n        -1.2832e+02,  2.3291e+02, -3.5308e+02,  1.7936e+01, -1.5868e+02,\n         8.3503e+01, -1.4901e+02, -2.6377e+01, -2.3254e+02, -1.2587e+02,\n        -5.9691e+02,  1.1212e+02,  6.1595e-02, -5.8180e+02,  3.8849e+02,\n         3.7292e+01,  4.1571e+02, -7.4397e+02, -2.9722e+02, -1.7181e+02,\n        -1.2449e+02, -2.2961e+02,  1.0642e+02, -2.8283e+02,  1.1253e+02,\n         1.4819e+02, -2.5120e+02, -1.8256e+02, -3.1537e+02, -3.5919e+02,\n        -1.3727e+01, -2.5256e+02, -3.3951e+02,  4.8042e+02, -3.7319e+02,\n         5.1413e+02, -5.6166e+02, -7.9882e+01,  2.6003e+02, -2.2188e+02,\n        -1.0177e+02,  3.5271e+02,  2.3384e+02,  6.0992e+01, -5.5559e+01,\n         1.9678e+02, -3.2434e+02,  3.3622e+01,  3.8088e+01, -3.0266e+02,\n        -1.8918e+02, -3.1818e+01,  1.4336e+02,  1.5760e+02,  2.4749e+01,\n        -3.7001e+02, -2.1663e+02, -7.0489e+01,  6.7122e+02,  6.9710e+01,\n         3.0499e+02,  4.1045e+02,  3.3742e+02,  2.3481e+02, -4.1465e+02,\n        -3.8656e+02,  5.3959e+01, -9.5932e+01,  4.0913e+02, -3.4359e+01,\n        -1.3261e+02, -4.2297e+02, -1.8751e+02,  8.1490e+00,  7.9376e+01,\n         2.1205e+01, -8.7214e+01,  2.1350e+02,  9.6179e+01, -4.4604e+02,\n        -5.2157e+02, -7.2023e+01,  2.8809e+02,  2.1576e+02,  4.1632e+02,\n        -4.5931e+02,  7.3226e+00, -1.4251e+02,  1.7137e+02,  2.1584e+02,\n         8.0312e+01,  1.8427e+02, -3.5661e+02, -2.5036e+02, -3.0235e+02,\n         1.4116e+02, -5.0341e+02, -1.1218e+01,  2.1780e+02,  1.6231e+02,\n         4.0349e+02,  2.4140e+02, -5.2571e+02, -5.1638e+02, -5.5183e+02,\n        -4.1269e+02, -8.4020e+02,  1.4413e+02, -3.4188e+01, -4.4450e+02,\n        -5.9636e+02,  9.4113e+01,  5.7062e+01,  2.3626e+02, -4.2296e+02,\n         3.2550e+02,  4.2906e+02,  4.0225e+01,  1.9970e+02, -2.9536e+00,\n         2.1472e+01,  6.4368e+01, -4.4824e+01, -1.0677e+03, -2.2907e+01,\n         2.8409e+02,  7.2713e+01,  1.3117e+02,  7.0831e+01,  1.7393e+02,\n         2.6355e+02, -2.8288e+02, -5.7320e+01, -1.4530e+02,  1.8223e+02,\n         2.8651e+01, -1.6598e+02,  3.6831e+01,  1.0323e+02,  2.1919e+02,\n        -6.9993e+01, -7.4922e+01,  1.4479e+02, -1.2069e+02,  2.8983e+02,\n        -6.7180e+01,  1.0231e+02, -1.6203e+02,  1.9782e+02, -1.6038e+02,\n        -2.5145e+02,  4.0866e+02,  3.1632e+02,  1.6956e+02, -1.3674e+02,\n        -4.7412e+01, -3.5761e+01,  1.7170e+02,  5.0292e+02, -5.1491e+01,\n         3.5725e+02,  1.0711e+02,  2.8948e+02,  1.2529e+02,  8.8769e+01,\n        -2.3818e+01, -3.3957e+02, -2.2694e+02, -1.5477e+02,  1.9022e+02,\n         2.5652e+02, -3.5102e+02, -5.3961e+02, -2.5266e+02, -1.3476e+02,\n        -2.2623e+02, -4.6485e+02, -1.7087e+02,  1.7974e+02,  1.4829e+02,\n         2.1216e+02, -5.7253e+02, -6.8831e+01, -3.7502e+02,  3.3081e+02,\n        -9.5422e+01,  1.4420e+02, -1.9829e+02,  9.2289e+02, -3.2843e+02,\n         2.5884e+01, -2.6407e+02, -4.6668e+02, -1.7423e+02,  5.1035e+02,\n        -1.5380e+02,  9.6053e+01,  1.1902e+02, -3.7858e+02,  2.1505e+02,\n        -2.2044e+02,  3.4774e+02, -2.5803e+02, -3.2429e+00, -1.4174e+02,\n         2.7708e+02, -1.0084e+02,  1.9423e+02,  9.7486e+01,  3.1458e+02,\n        -3.3661e+00, -2.2021e+02,  2.8919e+02,  9.8252e+01, -2.8699e+02,\n         2.5109e+02,  9.1454e+01,  5.6747e+01,  2.4632e+02, -3.7540e+02,\n         9.1398e+00,  5.3820e+01,  4.2743e+02,  3.4381e+02, -1.0348e+02,\n        -4.4070e+02,  1.2836e+02, -1.6284e+02, -1.3297e+02, -6.4355e+01,\n        -5.2239e+02,  8.1619e+01,  3.2284e+02,  2.8020e+01,  2.5904e+02,\n         1.9759e+02,  1.1258e+02, -1.9020e+02, -1.8468e+02,  9.1500e+01,\n         3.2304e+01,  1.0598e+02, -5.7936e+01,  1.6846e+02,  2.9682e+02,\n        -2.3748e+02,  3.5622e+02, -1.0048e+02, -8.8013e+01, -8.0007e+01,\n         3.8925e+02, -3.2770e+01,  4.6019e+01, -5.7634e+01,  4.4842e+02,\n        -1.6580e+02, -2.8305e+02, -5.0545e+02,  1.9385e+02, -2.8786e+02,\n         1.7045e+02, -2.7340e+02, -2.3974e+02,  5.8846e+02, -2.3319e+02,\n        -1.9619e+02, -5.1923e+01, -2.2796e+02, -5.8598e+02,  3.6916e+02,\n        -7.5199e+00,  9.2594e+01,  7.8712e+01,  2.5771e+01,  5.6311e+02,\n         6.2808e+02,  1.1905e+02,  5.8302e+01, -2.3574e+02, -1.7128e+02,\n        -2.7178e+01, -4.8334e+01,  1.9204e+02, -3.5789e+02, -2.5134e+02,\n        -1.5711e+01, -3.4543e+02,  2.0201e+02, -7.7125e+01,  8.8917e+01,\n         3.3458e+02,  2.5693e+02, -1.1466e+02, -4.8138e+02, -1.1061e+02,\n         1.4103e+02,  2.0248e+02, -4.6693e+02,  1.0515e+02,  3.1382e+02,\n         7.1360e+02,  1.6668e+02,  2.6694e+02, -2.0838e+02,  1.2243e+02,\n        -3.7100e+02, -1.3930e+02, -1.2251e+02, -8.3206e+01, -2.8321e+02,\n         1.3937e+02, -2.4800e+02,  3.0904e+02,  3.9903e+02,  6.0665e+01,\n        -1.3783e+02,  1.1732e+02, -1.6002e+02,  2.5998e+02,  5.6102e+01,\n        -7.1487e+00, -1.7955e+02, -2.7303e+02,  8.9997e+01, -2.8676e+02,\n         1.1971e+02,  2.2701e+02,  2.0217e+02,  3.4362e+02,  2.0539e+02,\n         4.5421e+02, -9.1052e+01,  5.0561e+01,  1.2689e+02, -1.3639e+02,\n         1.2318e+02, -3.5519e+02, -4.6309e+02,  2.6260e+02, -5.0719e+00,\n        -2.1670e+02,  1.4080e+02, -4.1876e+02, -1.7712e+02, -1.0961e+02,\n         2.1117e+02,  6.0087e+02,  1.5979e+02, -3.1308e+02,  3.8196e+01,\n         3.3052e+02, -1.5378e+02, -3.7169e+02,  1.0073e+02, -4.4186e+01,\n        -5.2283e+02,  2.7499e+02,  3.7366e+02,  5.0785e+01, -1.8691e+02,\n         1.0300e+00,  4.4110e+02,  1.9848e+02,  3.9934e+02,  2.2982e+02,\n         3.5545e+02,  2.2001e+02,  2.3101e+02, -1.5403e+02, -4.1422e+02,\n        -1.1263e+02,  1.0499e+02,  3.0001e+02,  1.3146e+02,  4.5125e+02,\n        -3.7438e+02,  7.9137e+00, -1.3655e+01, -1.2665e+02, -6.5342e+01,\n         8.5399e+02,  6.5139e+01, -7.1417e+01,  3.9979e+02, -1.6059e+02,\n        -5.5508e+02, -1.0705e+02,  1.5273e+02,  1.8095e+02, -1.0104e+02,\n         5.2482e+02,  2.1674e+02, -2.6014e+02,  1.6277e+02, -4.4570e+02,\n        -2.4823e+02,  2.4940e+02, -3.1398e+02, -8.5457e+01,  2.2945e+02,\n         3.1343e+02,  2.9559e+02,  3.3557e+02,  2.8838e+01,  3.0616e+02,\n         8.8445e+01,  1.0081e+01,  2.1245e+02,  9.3967e+01,  9.4806e+00])\nPARAMS FOR W :  1000000\nPARAMS FOR LORDECOMPOSTION :  200000\n","output_type":"stream"}]},{"cell_type":"code","source":"#LORA FOR SINGLE DIGIT FINETUNING ON A MNIST CLASSIFICATION TASK \nimport torchvision.datasets as datasets \nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:46:03.695966Z","iopub.execute_input":"2024-06-11T18:46:03.696325Z","iopub.status.idle":"2024-06-11T18:46:06.237342Z","shell.execute_reply.started":"2024-06-11T18:46:03.696293Z","shell.execute_reply":"2024-06-11T18:46:06.236556Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\nmnist_trainset = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\ntrain_loader = __T__.utils.data.DataLoader(mnist_trainset, batch_size = 10, shuffle = True)\nmnist_testset = datasets.MNIST(root = './data', train = False, download = True, transform = transform)\ntest_loader = __T__.utils.data.DataLoader(mnist_testset, batch_size = 10, shuffle = True)\ndevice = __T__.device(\"cuda:0\" if __T__.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:46:09.939849Z","iopub.execute_input":"2024-06-11T18:46:09.940564Z","iopub.status.idle":"2024-06-11T18:46:12.310187Z","shell.execute_reply.started":"2024-06-11T18:46:09.940530Z","shell.execute_reply":"2024-06-11T18:46:12.309216Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 34062035.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 1084609.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 10100158.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 3154061.05it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class RichNet(nn.Module):\n    def __init__(self, hidden_size_1 : int = 1000, hidden_size_2 : int = 2000):\n        super(RichNet,self).__init__()\n        self.linear1 = nn.Linear(28 * 28, hidden_size_1) \n        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n        self.linear3 = nn.Linear(hidden_size_2, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, img):\n        x = img.view(-1, 28 * 28)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        x = self.linear3(x)\n        return x\n\nnet = RichNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:46:18.176677Z","iopub.execute_input":"2024-06-11T18:46:18.177024Z","iopub.status.idle":"2024-06-11T18:46:18.379793Z","shell.execute_reply.started":"2024-06-11T18:46:18.176996Z","shell.execute_reply":"2024-06-11T18:46:18.378767Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, net, __epochs : int = 5, TOTAL_ITER_LIMIT = None) -> None:\n    cross_entropy_loss = nn.CrossEntropyLoss()\n    optimizer = __T__.optim.Adam(net.parameters(), lr = 0.001)\n\n    __total_iters : int = 0\n\n    for __epoch in range(__epochs):\n        net.train()\n        loss_sum : float = 0\n        __iter_num : int = 0\n\n        data_iterator = tqdm(train_loader, desc = f'Epoch {__epoch + 1}')\n        if TOTAL_ITER_LIMIT is not None: data_iterator.total = TOTAL_ITER_LIMIT\n        for data in data_iterator:\n            __iter_num += 1\n            __total_iters += 1\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            output = net(x.view(-1, 28 * 28))\n            loss = cross_entropy_loss(output, y)\n            loss_sum += loss.item()\n            avg_loss = loss_sum / __iter_num\n            data_iterator.set_postfix(loss = avg_loss)\n            loss.backward()\n            optimizer.step()\n\n            if TOTAL_ITER_LIMIT is not None and __total_iters >= TOTAL_ITER_LIMIT: return\n\ntrain(train_loader, net, __epochs = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:51:01.179462Z","iopub.execute_input":"2024-06-11T18:51:01.180304Z","iopub.status.idle":"2024-06-11T18:51:32.989894Z","shell.execute_reply.started":"2024-06-11T18:51:01.180268Z","shell.execute_reply":"2024-06-11T18:51:32.988984Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 6000/6000 [00:31<00:00, 188.70it/s, loss=0.124]\n","output_type":"stream"}]},{"cell_type":"code","source":"original_weights : Dict[str, nn.parameter.Parameter] = {}\nfor name, param in net.named_parameters(): original_weights[name] = param.clone().detach()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:59:32.568787Z","iopub.execute_input":"2024-06-11T18:59:32.569818Z","iopub.status.idle":"2024-06-11T18:59:32.575545Z","shell.execute_reply.started":"2024-06-11T18:59:32.569770Z","shell.execute_reply":"2024-06-11T18:59:32.574608Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def test():\n    correct : int = 0\n    total : int = 0\n    wrong_counts = [0 for i in range(10)]\n\n    with __T__.no_grad():\n        for data in tqdm(test_loader, desc='Testing'):\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            output = net(x.view(-1, 784))\n            for idx, i in enumerate(output):\n                if __T__.argmax(i) == y[idx] : correct += 1\n                else : wrong_counts[y[idx]] += 1\n                total += 1\n    print(f'Accuracy: {round(correct / total, 3)}')\n    for i in range(len(wrong_counts)):\n        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T18:59:43.840791Z","iopub.execute_input":"2024-06-11T18:59:43.841376Z","iopub.status.idle":"2024-06-11T18:59:47.210409Z","shell.execute_reply.started":"2024-06-11T18:59:43.841341Z","shell.execute_reply":"2024-06-11T18:59:47.209263Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:03<00:00, 297.83it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.963\nwrong counts for the digit 0: 26\nwrong counts for the digit 1: 28\nwrong counts for the digit 2: 31\nwrong counts for the digit 3: 65\nwrong counts for the digit 4: 33\nwrong counts for the digit 5: 15\nwrong counts for the digit 6: 27\nwrong counts for the digit 7: 49\nwrong counts for the digit 8: 22\nwrong counts for the digit 9: 74\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the size of the weights matrices of the network\n# Save the count of the total number of parameters\nTOTAL_ORG_PARAMS : int = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    TOTAL_ORG_PARAMS += layer.weight.nelement() + layer.bias.nelement()\n    print(f'Layer {index + 1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\nprint(f'Total number of parameters: {TOTAL_ORG_PARAMS:,}')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:00:04.506707Z","iopub.execute_input":"2024-06-11T19:00:04.507071Z","iopub.status.idle":"2024-06-11T19:00:04.513498Z","shell.execute_reply.started":"2024-06-11T19:00:04.507040Z","shell.execute_reply":"2024-06-11T19:00:04.512562Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\nTotal number of parameters: 2,807,010\n","output_type":"stream"}]},{"cell_type":"code","source":"class LoRAParametrization(nn.Module):\n    def __init__(self, features_in, features_out, rank = 1, alpha = 1, device = 'cpu'):\n        super().__init__()\n        # Section 4.1 of the paper: \n        #   We use a random Gaussian initialization for A and zero for B, so \n        #   ∆W = BA is zero at the beginning of training\n        self.lora_A = nn.Parameter(__T__.zeros((rank,features_out)).to(device))\n        self.lora_B = nn.Parameter(__T__.zeros((features_in, rank)).to(device))\n        nn.init.normal_(self.lora_A, mean = 0, std = 1)\n        \n        # Section 4.1 of the paper: \n        #   We then scale ∆Wx by α/r , where α is a constant in r. \n        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning \n        #   rate if we scale the initialization appropriately. \n        #   As a result, we simply set α to the first r we try and do not tune it. \n        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n        self.scale = alpha / rank\n        self.enabled = True\n\n    def forward(self, original_weights):\n        if self.enabled:\n            # Return W + (B*A)*scale\n            return original_weights + __T__.matmul(self.lora_B, \\\n                                                   self.lora_A).view(original_weights.shape) * self.scale\n        else: return original_weights","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:06:21.568957Z","iopub.execute_input":"2024-06-11T19:06:21.569612Z","iopub.status.idle":"2024-06-11T19:06:21.577766Z","shell.execute_reply.started":"2024-06-11T19:06:21.569561Z","shell.execute_reply":"2024-06-11T19:06:21.576683Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#Adding the parametrization to the net\nimport torch.nn.utils.parametrize as parametrize\n\ndef linear_layer_parameterization(layer, device, rank = 1, lora_alpha = 1):\n    # Only add the parameterization to the weight matrix, ignore the Bias\n    # From section 4.2 of the paper:\n    #   We limit our study to only adapting the attention weights for downstream tasks and \n    #   freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and \n    #   parameter-efficiency.\n    \n    features_in, features_out = layer.weight.shape\n    return LoRAParametrization(\n        features_in, features_out, rank = rank, alpha = lora_alpha, device = device\n    )\n\nparametrize.register_parametrization(\n    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n)\nparametrize.register_parametrization(\n    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n)\nparametrize.register_parametrization(\n    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n)\n\n\ndef enable_disable_lora(enabled = True):\n    for layer in [net.linear1, net.linear2, net.linear3]:\n        layer.parametrizations[\"weight\"][0].enabled = enabled","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:06:24.531415Z","iopub.execute_input":"2024-06-11T19:06:24.531796Z","iopub.status.idle":"2024-06-11T19:06:24.594307Z","shell.execute_reply.started":"2024-06-11T19:06:24.531768Z","shell.execute_reply":"2024-06-11T19:06:24.593547Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"TOTAL_PARAMS_LORA : int = 0\nTOTAL_PARAMS_NON_LORA : int = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    TOTAL_PARAMS_LORA += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n    TOTAL_PARAMS_NON_LORA += layer.weight.nelement() + layer.bias.nelement()\n    print(\n        f'Layer {index + 1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n    )\n# The non-LoRA parameters count must match the original network\nassert TOTAL_PARAMS_NON_LORA == TOTAL_ORG_PARAMS\nprint(f'Total number of parameters (original): {TOTAL_PARAMS_NON_LORA:,}')\nprint(f'Total number of parameters (original + LoRA): {TOTAL_PARAMS_LORA + TOTAL_PARAMS_NON_LORA:,}')\nprint(f'Parameters introduced by LoRA: {TOTAL_PARAMS_LORA:,}')\nparameters_increment = (TOTAL_PARAMS_LORA / TOTAL_PARAMS_NON_LORA) * 100\nprint(f'Parameters incremment: {parameters_increment:.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:12:02.262678Z","iopub.execute_input":"2024-06-11T19:12:02.263066Z","iopub.status.idle":"2024-06-11T19:12:02.274648Z","shell.execute_reply.started":"2024-06-11T19:12:02.263035Z","shell.execute_reply":"2024-06-11T19:12:02.273579Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\nTotal number of parameters (original): 2,807,010\nTotal number of parameters (original + LoRA): 2,813,804\nParameters introduced by LoRA: 6,794\nParameters incremment: 0.242%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Freeze the non-Lora parameters\nfor name, param in net.named_parameters():\n    if 'lora' not in name:\n        print(f'Freezing non-LoRA parameter {name}')\n        param.requires_grad = False\n\n# Load the MNIST dataset again, by keeping only the digit 9\nmnist_trainset = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\nexclude_indices = mnist_trainset.targets == 9\nmnist_trainset.data = mnist_trainset.data[exclude_indices]\nmnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n# Create a dataloader for the training\ntrain_loader = __T__.utils.data.DataLoader(mnist_trainset, batch_size = 10, shuffle = True)\n\n# Train the network with LoRA only on the digit 9 and only for 100 batches\ntrain(train_loader, net, __epochs = 1, TOTAL_ITER_LIMIT = 100)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:38:47.824975Z","iopub.execute_input":"2024-06-11T19:38:47.825691Z","iopub.status.idle":"2024-06-11T19:38:48.538914Z","shell.execute_reply.started":"2024-06-11T19:38:47.825658Z","shell.execute_reply":"2024-06-11T19:38:48.537976Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Freezing non-LoRA parameter linear1.bias\nFreezing non-LoRA parameter linear1.parametrizations.weight.original\nFreezing non-LoRA parameter linear2.bias\nFreezing non-LoRA parameter linear2.parametrizations.weight.original\nFreezing non-LoRA parameter linear3.bias\nFreezing non-LoRA parameter linear3.parametrizations.weight.original\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 159.24it/s, loss=0.0457]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check that the frozen parameters are still unchanged by the finetuning\nassert __T__.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\nassert __T__.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\nassert __T__.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n\nenable_disable_lora(enabled = True)\n# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n# The original weights have been moved to net.linear1.parametrizations.weight.original\n# https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\nassert __T__.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n\nenable_disable_lora(enabled = False)\n# If we disable LoRA, the linear1.weight is the original one\nassert __T__.equal(net.linear1.weight, original_weights['linear1.weight'])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:40:03.342834Z","iopub.execute_input":"2024-06-11T19:40:03.343497Z","iopub.status.idle":"2024-06-11T19:40:03.370401Z","shell.execute_reply.started":"2024-06-11T19:40:03.343452Z","shell.execute_reply":"2024-06-11T19:40:03.369387Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Test with LoRA enabled\nenable_disable_lora(enabled = True)\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:40:22.878038Z","iopub.execute_input":"2024-06-11T19:40:22.878401Z","iopub.status.idle":"2024-06-11T19:40:26.679401Z","shell.execute_reply.started":"2024-06-11T19:40:22.878369Z","shell.execute_reply":"2024-06-11T19:40:26.678412Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:03<00:00, 263.64it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.925\nwrong counts for the digit 0: 46\nwrong counts for the digit 1: 78\nwrong counts for the digit 2: 48\nwrong counts for the digit 3: 112\nwrong counts for the digit 4: 114\nwrong counts for the digit 5: 45\nwrong counts for the digit 6: 57\nwrong counts for the digit 7: 85\nwrong counts for the digit 8: 158\nwrong counts for the digit 9: 8\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test with LoRA disabled\nenable_disable_lora(enabled = False)\ntest()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T19:40:44.108085Z","iopub.execute_input":"2024-06-11T19:40:44.108482Z","iopub.status.idle":"2024-06-11T19:40:47.598710Z","shell.execute_reply.started":"2024-06-11T19:40:44.108453Z","shell.execute_reply":"2024-06-11T19:40:47.597827Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:03<00:00, 287.17it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.963\nwrong counts for the digit 0: 26\nwrong counts for the digit 1: 28\nwrong counts for the digit 2: 31\nwrong counts for the digit 3: 65\nwrong counts for the digit 4: 33\nwrong counts for the digit 5: 15\nwrong counts for the digit 6: 27\nwrong counts for the digit 7: 49\nwrong counts for the digit 8: 22\nwrong counts for the digit 9: 74\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}